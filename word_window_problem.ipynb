{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's dataset\n",
    "corpus = [\n",
    "          \"We always come to Paris\",\n",
    "          \"The professor is from Australia\",\n",
    "          \"I live in Stanford\",\n",
    "          \"He comes from Taiwan\",\n",
    "          \"The capital of Turkey is Ankara\"\n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the dataset for bare-minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['we', 'always', 'come', 'to', 'paris'],\n",
       " ['the', 'professor', 'is', 'from', 'australia'],\n",
       " ['i', 'live', 'in', 'stanford'],\n",
       " ['he', 'comes', 'from', 'taiwan'],\n",
       " ['the', 'capital', 'of', 'turkey', 'is', 'ankara']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowercasing the code\n",
    "def preprocess_sentence(sentence) -> str:\n",
    "    return sentence.lower().split() \n",
    "\n",
    "# creating the training set\n",
    "train_sentences = [preprocess_sentence(text) for text in corpus]\n",
    "train_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set locations to apperar in corpus\n",
    "locations = set([\"australia\", \"ankara\", \"paris\", \"stanford\", \"taiwan\", \"turkey\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1, 0, 1]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training labels\n",
    "train_labels = [[1 if word in locations else 0 for word in text] for text in train_sentences]\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings! Fuck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'always',\n",
       " 'ankara',\n",
       " 'australia',\n",
       " 'capital',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'from',\n",
       " 'he',\n",
       " 'i',\n",
       " 'in',\n",
       " 'is',\n",
       " 'live',\n",
       " 'of',\n",
       " 'paris',\n",
       " 'professor',\n",
       " 'stanford',\n",
       " 'taiwan',\n",
       " 'the',\n",
       " 'to',\n",
       " 'turkey',\n",
       " 'we'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = set(w for s in train_sentences for w in s)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the unknown token to our vocabulary\n",
    "vocab.add(\"<unk>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '<pad>', 'we', 'always', 'come', 'to', 'paris', '<pad>', '<pad>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.add(\"<pad>\")\n",
    "def pad_window(sentence, window_size, pad_tokens=\"<pad>\"):\n",
    "    window = [pad_tokens] * window_size\n",
    "    return window + sentence + window\n",
    "\n",
    "window_size = 2\n",
    "pad_window(train_sentences[0], window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<unk>': 1,\n",
       " 'always': 2,\n",
       " 'ankara': 3,\n",
       " 'australia': 4,\n",
       " 'capital': 5,\n",
       " 'come': 6,\n",
       " 'comes': 7,\n",
       " 'from': 8,\n",
       " 'he': 9,\n",
       " 'i': 10,\n",
       " 'in': 11,\n",
       " 'is': 12,\n",
       " 'live': 13,\n",
       " 'of': 14,\n",
       " 'paris': 15,\n",
       " 'professor': 16,\n",
       " 'stanford': 17,\n",
       " 'taiwan': 18,\n",
       " 'the': 19,\n",
       " 'to': 20,\n",
       " 'turkey': 21,\n",
       " 'we': 22}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have to assign an idx to each vocab\n",
    "ix_to_word = sorted(list(vocab))\n",
    "word_to_ix = {word: ind for ind, word in enumerate(ix_to_word)}\n",
    "word_to_ix\n",
    "# enumerate lets us keep track something like fuck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert them to indiuces\n",
    "def convert_token_to_indices(sentence, word_to_ix):\n",
    "    indices = []\n",
    "    for token in sentence:\n",
    "        if token in word_to_ix:\n",
    "            index = word_to_ix[token]\n",
    "        else:\n",
    "            index = word_to_ix[\"<unk>\"]\n",
    "        indices.append(index)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = [\"we\", \"always\", \"come\", \"to\", \"kuwait\"]\n",
    "example_indices = convert_token_to_indices(example_sentence, word_to_ix)\n",
    "restored_example = [ix_to_word[ind] for ind in example_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence is: ['we', 'always', 'come', 'to', 'kuwait']\n",
      "Going from words to indices: [22, 2, 6, 20, 1]\n",
      "Going from indices to words: ['we', 'always', 'come', 'to', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original sentence is: {example_sentence}\")\n",
    "print(f\"Going from words to indices: {example_indices}\")\n",
    "print(f\"Going from indices to words: {restored_example}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_padded_indices = [convert_token_to_indices(s,word_to_ix) for s in train_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[22, 2, 6, 20, 15],\n",
       " [19, 16, 12, 8, 4],\n",
       " [10, 13, 11, 17],\n",
       " [9, 7, 8, 18],\n",
       " [19, 5, 14, 21, 12, 3]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_padded_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the embedding table\n",
    "embedding_dim = 5\n",
    "import torch.nn as nn\n",
    "embeds = nn.Embedding(len(vocab), embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emperical Formula for determining Embedding dim\n",
    "\n",
    "Embed_dim = (num_of_word_or_elements ** 0.25) # or NUM OF ELEMENTS ^(1/2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.5686,  0.9715,  0.0451,  1.5931, -1.4669],\n",
       "         [-0.5358, -1.2165,  0.3604, -0.4787, -1.4749],\n",
       "         [ 2.5492, -0.2397, -0.1668,  0.5925,  0.6002],\n",
       "         [-0.1383,  0.9898, -0.4856,  1.3780,  0.2166],\n",
       "         [ 0.4921,  0.1460,  0.8977, -1.2567, -0.3184],\n",
       "         [ 1.4324, -0.1228,  1.3989, -0.0445,  0.1734],\n",
       "         [-1.0744,  1.0244, -0.3263,  1.8729, -0.4322],\n",
       "         [ 0.2626, -0.7837,  0.8404, -0.6829,  1.2088],\n",
       "         [ 0.2452,  1.4278, -0.0186, -0.1464, -0.6267],\n",
       "         [ 2.1342,  1.3594,  0.3160, -0.0366, -1.5533],\n",
       "         [ 0.8921,  1.8061,  0.4301, -1.1312,  0.6416],\n",
       "         [-1.4366,  0.5967,  1.0330,  0.3859,  1.1610],\n",
       "         [-1.2120,  0.9837, -0.0352, -0.5095,  0.7749],\n",
       "         [ 0.2758,  0.2709, -0.2766,  1.6259,  0.0664],\n",
       "         [ 0.6223,  1.3239,  0.9706, -0.4761, -0.6280],\n",
       "         [ 0.5972, -2.5759, -0.3597,  0.6420, -0.2552],\n",
       "         [ 1.2539, -0.3473,  2.4494, -0.0641, -1.0340],\n",
       "         [ 0.6382, -0.4149, -2.1569,  0.4442, -0.3501],\n",
       "         [ 0.2580,  0.6898,  1.3691,  0.4921, -0.2556],\n",
       "         [-1.5031,  0.7065, -1.7803,  0.3059, -1.9225],\n",
       "         [-0.7825,  0.6537, -0.7897,  0.7895,  0.2909],\n",
       "         [ 0.4626,  0.2906, -0.4292,  1.0946, -0.1611],\n",
       "         [ 0.5192, -1.0967, -0.4050,  0.0672, -0.3915]], requires_grad=True)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(embeds.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5972, -2.5759, -0.3597,  0.6420, -0.2552],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the embeddings for the word Paris\n",
    "index = word_to_ix[\"paris\"]\n",
    "import torch \n",
    "index_tensor = torch.tensor(index, dtype=torch.long)\n",
    "paris_embed = embeds(index_tensor)\n",
    "paris_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5972, -2.5759, -0.3597,  0.6420, -0.2552],\n",
       "        [-0.1383,  0.9898, -0.4856,  1.3780,  0.2166]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also get multiple embeddings at once\n",
    "index_paris = word_to_ix[\"paris\"]\n",
    "index_ankara = word_to_ix[\"ankara\"]\n",
    "indices = [index_paris, index_ankara]\n",
    "indices_tensor = torch.tensor(indices, dtype=torch.long)\n",
    "embeddings = embeds(indices_tensor)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch, window_size, word_to_ix):\n",
    "    x, y = zip(*batch)\n",
    "    x = [pad_window(s, window_size=window_size) for s in x]\n",
    "    x = [convert_token_to_indices(s, word_to_ix) for s in x]\n",
    "    \n",
    "    ## pading so that it has all sentences with same size\n",
    "    pad_token_ix =  word_to_ix[\"<pad>\"]\n",
    "    x = [torch.LongTensor(x_i) for x_i in x]\n",
    "    x_padded = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=pad_token_ix)\n",
    "    \n",
    "    # batch first means batch size will be first and then seq_len\n",
    "    \n",
    "    # pad y and record the length \n",
    "    lengths = [len(label) for label in y]\n",
    "    lengths = torch.LongTensor(lengths)\n",
    "    y = [torch.LongTensor(y_i) for y_i in y]\n",
    "    y_padded = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return x_padded, y_padded, lengths\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(zip(train_sentences, train_labels))\n",
    "batch_size = 5\n",
    "shuffle = True \n",
    "window_size = 2\n",
    "from functools import partial\n",
    "collate_fn = partial(custom_collate_fn, window_size=window_size, word_to_ix=word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader; loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Batched INPUT: \n",
      "tensor([[ 0,  0,  9,  7,  8, 18,  0,  0,  0,  0],\n",
      "        [ 0,  0, 22,  2,  6, 20, 15,  0,  0,  0],\n",
      "        [ 0,  0, 19,  5, 14, 21, 12,  3,  0,  0],\n",
      "        [ 0,  0, 19, 16, 12,  8,  4,  0,  0,  0],\n",
      "        [ 0,  0, 10, 13, 11, 17,  0,  0,  0,  0]])\n",
      "Batched Labels: \n",
      "tensor([[0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 1, 0, 1],\n",
      "        [0, 0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 1, 0, 0]])\n",
      "Batched lengths: \n",
      "tensor([4, 5, 6, 5, 4])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for batched_x, batched_y, batched_lengths in loader:\n",
    "    print(f'Iteration {counter}')\n",
    "    print(f'Batched INPUT: ')\n",
    "    print(batched_x)\n",
    "    print(\"Batched Labels: \")\n",
    "    print(batched_y)\n",
    "    print(\"Batched lengths: \")\n",
    "    print(batched_lengths)\n",
    "    print(\"\")\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original tensor\n",
      "tensor([[ 0,  0,  9,  7,  8, 18,  0,  0,  0,  0],\n",
      "        [ 0,  0, 22,  2,  6, 20, 15,  0,  0,  0],\n",
      "        [ 0,  0, 19,  5, 14, 21, 12,  3,  0,  0],\n",
      "        [ 0,  0, 19, 16, 12,  8,  4,  0,  0,  0],\n",
      "        [ 0,  0, 10, 13, 11, 17,  0,  0,  0,  0]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printing the original tensor \n",
    "print(\"original tensor\")\n",
    "print(batched_x)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windows: \n",
      "tensor([[[ 0,  0,  9,  7,  8],\n",
      "         [ 0,  9,  7,  8, 18],\n",
      "         [ 9,  7,  8, 18,  0],\n",
      "         [ 7,  8, 18,  0,  0],\n",
      "         [ 8, 18,  0,  0,  0],\n",
      "         [18,  0,  0,  0,  0]],\n",
      "\n",
      "        [[ 0,  0, 22,  2,  6],\n",
      "         [ 0, 22,  2,  6, 20],\n",
      "         [22,  2,  6, 20, 15],\n",
      "         [ 2,  6, 20, 15,  0],\n",
      "         [ 6, 20, 15,  0,  0],\n",
      "         [20, 15,  0,  0,  0]],\n",
      "\n",
      "        [[ 0,  0, 19,  5, 14],\n",
      "         [ 0, 19,  5, 14, 21],\n",
      "         [19,  5, 14, 21, 12],\n",
      "         [ 5, 14, 21, 12,  3],\n",
      "         [14, 21, 12,  3,  0],\n",
      "         [21, 12,  3,  0,  0]],\n",
      "\n",
      "        [[ 0,  0, 19, 16, 12],\n",
      "         [ 0, 19, 16, 12,  8],\n",
      "         [19, 16, 12,  8,  4],\n",
      "         [16, 12,  8,  4,  0],\n",
      "         [12,  8,  4,  0,  0],\n",
      "         [ 8,  4,  0,  0,  0]],\n",
      "\n",
      "        [[ 0,  0, 10, 13, 11],\n",
      "         [ 0, 10, 13, 11, 17],\n",
      "         [10, 13, 11, 17,  0],\n",
      "         [13, 11, 17,  0,  0],\n",
      "         [11, 17,  0,  0,  0],\n",
      "         [17,  0,  0,  0,  0]]])\n"
     ]
    }
   ],
   "source": [
    "# create the 2 *2 + 1 chunks \n",
    "chunk = batched_x.unfold(1, window_size * 2 + 1, 1)\n",
    "print(f\"Windows: \")\n",
    "print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordWindowClassifier(nn.Module):\n",
    "    def __init__(self, hyperparameters, vocab_size, pad_ix=0):\n",
    "        super().__init__()\n",
    "        self.window_size = hyperparameters[\"window_size\"]\n",
    "        self.embed_dim = hyperparameters[\"embed_dim\"]\n",
    "        self.hidden_dim = hyperparameters[\"hidden_dim\"]\n",
    "        self.freeze_embeddings = hyperparameters[\"freeze_embedding\"]\n",
    "        \n",
    "        self.embeds = nn.Embedding(vocab_size, self.embed_dim, padding_idx=pad_ix)\n",
    "        if self.freeze_embeddings:\n",
    "            self.embeds.weight.requires_grad = False\n",
    "\n",
    "        full_window_size = 2 * self.window_size + 1\n",
    "        self.hidden_layer = nn.Sequential(\n",
    "            nn.Linear(full_window_size * self.embed_dim, self.hidden_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(self.hidden_dim, 1)\n",
    "        self.probabilities = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        B, L = inputs.size()\n",
    "        token_windows = inputs.unfold(1, 2 * self.window_size + 1, 1)\n",
    "        _, adjusted_length, window_size = token_windows.size()\n",
    "        assert token_windows.size() == (B, adjusted_length, 2 * self.window_size + 1)\n",
    "\n",
    "        embedded_windows = self.embeds(token_windows)\n",
    "        embedded_windows = embedded_windows.view(B, adjusted_length, -1)\n",
    "        layer1 = self.hidden_layer(embedded_windows)\n",
    "        output = self.output_layer(layer1)\n",
    "        output = self.probabilities(output)\n",
    "        output = output.view(B, -1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(zip(train_sentences, train_labels))\n",
    "batch_size = 3\n",
    "shuffle = True\n",
    "window_size = 2\n",
    "collate_fn = partial(custom_collate_fn, window_size=window_size, word_to_ix=word_to_ix)\n",
    "\n",
    "# Instantiate a DataLoader\n",
    "loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize a model\n",
    "# It is useful to put all the model hyperparameters in a dictionary\n",
    "model_hyperparameters = {\n",
    "    \"batch_size\": 4,\n",
    "    \"window_size\": 2,\n",
    "    \"embed_dim\": 25,\n",
    "    \"hidden_dim\": 25,\n",
    "    \"freeze_embedding\": False,\n",
    "}\n",
    "\n",
    "vocab_size = len(word_to_ix)\n",
    "model = WordWindowClassifier(model_hyperparameters, vocab_size)\n",
    "\n",
    "# Define an optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(batch_outputs, batch_labels, batch_lengths):\n",
    "    bceloss = nn.BCELoss()\n",
    "    loss = bceloss(batch_outputs, batch_labels.float())\n",
    "    # rescale loss\n",
    "    # number of words in each training example\n",
    "    loss = loss / batch_lengths.sum().float()\n",
    "    return loss \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(loss_function, optimizer, model, loader):\n",
    "    # keep track of total loss for batch\n",
    "    total_loss = 0\n",
    "    for batch_inputs, batch_labels, batched_lengths in loader:\n",
    "        # clear gradients \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model.forward(batch_inputs)\n",
    "        loss = loss_function(outputs, batch_labels, batched_lengths)\n",
    "        # calculate gradients\n",
    "        loss.backward()\n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "def train(loss_function, optimizer, model, loader, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = train_epoch(loss_function, optimizer, model, loader)\n",
    "        if epoch % 100 == 0: print(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05682901665568352\n",
      "0.04459817335009575\n",
      "0.04280679300427437\n",
      "0.036663344129920006\n",
      "0.03145122807472944\n",
      "0.03407581336796284\n",
      "0.026276499032974243\n",
      "0.02902739029377699\n",
      "0.027926050126552582\n",
      "0.02056761644780636\n",
      "0.023901347536593676\n",
      "0.01599164167419076\n",
      "0.01676288153976202\n",
      "0.018284517340362072\n",
      "0.014100173022598028\n",
      "0.015149241778999567\n",
      "0.014220210257917643\n",
      "0.012395231053233147\n",
      "0.013274933211505413\n",
      "0.012863545212894678\n",
      "0.011986434925347567\n",
      "0.010154706425964832\n",
      "0.01146731711924076\n",
      "0.009751930832862854\n",
      "0.009300614707171917\n",
      "0.009944861056283116\n",
      "0.007339452393352985\n",
      "0.007013353751972318\n",
      "0.005826340522617102\n",
      "0.007340805605053902\n",
      "0.009023855673149228\n",
      "0.006835629232227802\n",
      "0.009304615436121821\n",
      "0.009014354320243001\n",
      "0.006179980002343655\n",
      "0.005101962597109377\n",
      "0.005879396805539727\n",
      "0.005633634282276034\n",
      "0.004003340960480273\n",
      "0.005936676636338234\n",
      "0.006601099856197834\n",
      "0.004188288934528828\n",
      "0.004063788102939725\n",
      "0.006868573487736285\n",
      "0.004578335676342249\n",
      "0.005040105897933245\n",
      "0.0036285044625401497\n",
      "0.0030596618307754397\n",
      "0.003439825610257685\n",
      "0.004006463917903602\n",
      "0.004003738868050277\n",
      "0.0031629434088245034\n",
      "0.004256748128682375\n",
      "0.0026277860160917044\n",
      "0.002971289912238717\n",
      "0.0029036421328783035\n",
      "0.003884752281010151\n",
      "0.004983699414879084\n",
      "0.002345975488424301\n",
      "0.004174931440502405\n",
      "0.004090684233233333\n",
      "0.0031332005746662617\n",
      "0.003069124650210142\n",
      "0.0021126120118424296\n",
      "0.0032813047291710973\n",
      "0.004224256437737495\n",
      "0.0036294647725299\n",
      "0.0035618554102256894\n",
      "0.0027368103619664907\n",
      "0.0029878733912482858\n",
      "0.0021036715479567647\n",
      "0.00281412061303854\n",
      "0.002548771910369396\n",
      "0.0017538524698466063\n",
      "0.0027452163631096482\n",
      "0.003540453384630382\n",
      "0.002574295795056969\n",
      "0.0023448921274393797\n",
      "0.003380605310667306\n",
      "0.002269144752062857\n",
      "0.0021638007601723075\n",
      "0.00280338479205966\n",
      "0.003157620958518237\n",
      "0.001492048380896449\n",
      "0.0023362402571365237\n",
      "0.0016811797395348549\n",
      "0.002965504943858832\n",
      "0.002557620406150818\n",
      "0.0028762049041688442\n",
      "0.002482893061824143\n",
      "0.00213589274790138\n",
      "0.0018861048156395555\n",
      "0.002028328424785286\n",
      "0.001294596353545785\n",
      "0.0026374171720817685\n",
      "0.001938511326443404\n",
      "0.002570042561274022\n",
      "0.002221018832642585\n",
      "0.0018750212620943785\n",
      "0.002461919153574854\n",
      "0.0024244862725026906\n",
      "0.002385792846325785\n",
      "0.0011539836414158344\n",
      "0.0013231217162683606\n",
      "0.001763613021466881\n",
      "0.0022656233923044056\n",
      "0.002236243075458333\n",
      "0.0016745103639550507\n",
      "0.0012089528900105506\n",
      "0.0014847202692180872\n",
      "0.001637898269109428\n",
      "0.001605635799933225\n",
      "0.0015887453337199986\n",
      "0.0014122910797595978\n",
      "0.0017941764672286808\n",
      "0.0015256911283358932\n",
      "0.0015071751549839973\n",
      "0.0013415523571893573\n",
      "0.0019317107798997313\n",
      "0.0019100101199001074\n",
      "0.0018856017559301108\n",
      "0.0009337232913821936\n",
      "0.0009240403305739164\n",
      "0.001623219985049218\n",
      "0.0012517735885921866\n",
      "0.0015905830659903586\n",
      "0.0012262571253813803\n",
      "0.0012113586417399347\n",
      "0.00100950978230685\n",
      "0.001187100657261908\n",
      "0.0011762249050661922\n",
      "0.001292675908189267\n",
      "0.001153220800915733\n",
      "0.0011414108448661864\n",
      "0.0011308977263979614\n",
      "0.0012447542685549706\n",
      "0.0009359446412418038\n",
      "0.000895544741069898\n",
      "0.0012106726644560695\n",
      "0.0008793615561444312\n",
      "0.0011991375940851867\n",
      "0.0011908203014172614\n",
      "0.0010519566130824387\n",
      "0.0007589903834741563\n",
      "0.00116021151188761\n",
      "0.0007461702916771173\n",
      "0.0014578433183487505\n",
      "0.0007336325652431697\n",
      "0.0009949554514605552\n",
      "0.0011068697785958648\n",
      "0.0014079647371545434\n",
      "0.001081237307516858\n",
      "0.0009624146041460335\n",
      "0.0007799581217113882\n",
      "0.0010552183084655553\n",
      "0.0007964541437104344\n",
      "0.001045776647515595\n",
      "0.0009277881472371519\n",
      "0.0010337913990952075\n",
      "0.0010137659264728427\n",
      "0.0010050245036836714\n",
      "0.0008927651797421277\n",
      "0.0008918730309233069\n",
      "0.00099358128500171\n",
      "0.001253997761523351\n",
      "0.0012429860071279109\n",
      "0.0009708669676911086\n",
      "0.0008520396950189024\n",
      "0.0006222352385520935\n",
      "0.0006176975730340928\n",
      "0.0008324543014168739\n",
      "0.0007056723989080638\n",
      "0.0007006263476796448\n",
      "0.0008135968237183988\n",
      "0.0006906760099809617\n",
      "0.0006592518766410649\n",
      "0.0009029692155309021\n",
      "0.0007985179254319519\n",
      "0.000645450985757634\n",
      "0.0005757444305345416\n",
      "0.0006366370362229645\n",
      "0.0008724083891138434\n",
      "0.0005640947492793202\n",
      "0.0008604419126641005\n",
      "0.0008469444292131811\n",
      "0.0007555063348263502\n",
      "0.0007422404014505446\n",
      "0.0008295652223750949\n",
      "0.0010509413259569556\n",
      "0.0005991319048916921\n",
      "0.0010371127573307604\n",
      "0.001030575076583773\n",
      "0.0008108811161946505\n",
      "0.0007086449768394232\n",
      "0.0005802893429063261\n",
      "0.0007842207851354033\n",
      "0.000996002825559117\n",
      "0.000774290063418448\n",
      "0.0005658988957293332\n",
      "0.0007667996105737984\n",
      "0.000502692797454074\n",
      "0.0007570924644824117\n",
      "0.0008603240130469203\n",
      "0.0007475654711015522\n",
      "0.0007405226933769882\n",
      "0.0006651481380686164\n",
      "0.0007316397968679667\n",
      "0.0007268522458616644\n",
      "0.0004790966777363792\n",
      "0.0004763137549161911\n",
      "0.0005260138568701223\n",
      "0.0005229105590842664\n",
      "0.0006290180754149333\n",
      "0.0007041825738269836\n",
      "0.0005364983517210931\n",
      "0.0007945400720927864\n",
      "0.0005303807702148333\n",
      "0.0008737079770071432\n",
      "0.000524493952980265\n",
      "0.000776271743234247\n",
      "0.0006764909776393324\n",
      "0.0006051204691175371\n",
      "0.0005931583000347018\n",
      "0.0005896933434996754\n",
      "0.00043807411566376686\n",
      "0.0004357719299150631\n",
      "0.0006507472426164895\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100000\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[40], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(loss_function, optimizer, model, loader, num_epochs)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(loss_function, optimizer, model, loader, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 18\u001b[0m         epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28mprint\u001b[39m(epoch_loss)\n",
      "Cell \u001b[1;32mIn[40], line 4\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(loss_function, optimizer, model, loader)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_epoch\u001b[39m(loss_function, optimizer, model, loader):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# keep track of total loss for batch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_inputs, batch_labels, batched_lengths \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;66;03m# clear gradients \u001b[39;00m\n\u001b[0;32m      6\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      7\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(batch_inputs)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 9\u001b[0m, in \u001b[0;36mcustom_collate_fn\u001b[1;34m(batch, window_size, word_to_ix)\u001b[0m\n\u001b[0;32m      7\u001b[0m pad_token_ix \u001b[38;5;241m=\u001b[39m  word_to_ix[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      8\u001b[0m x \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mLongTensor(x_i) \u001b[38;5;28;01mfor\u001b[39;00m x_i \u001b[38;5;129;01min\u001b[39;00m x]\n\u001b[1;32m----> 9\u001b[0m x_padded \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token_ix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# batch first means batch size will be first and then seq_len\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# pad y and record the length \u001b[39;00m\n\u001b[0;32m     14\u001b[0m lengths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(label) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m y]\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\utils\\rnn.py:478\u001b[0m, in \u001b[0;36mpad_sequence\u001b[1;34m(sequences, batch_first, padding_value, padding_side)\u001b[0m\n\u001b[0;32m    474\u001b[0m         sequences \u001b[38;5;241m=\u001b[39m sequences\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# assuming trailing dimensions and type of all the Tensors\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;66;03m# in sequences are same and fetching those from sequences[0]\u001b[39;00m\n\u001b[1;32m--> 478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_sequence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100000\n",
    "train(loss_function, optimizer, model, loader, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
